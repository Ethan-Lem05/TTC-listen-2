{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook \n",
    "\n",
    "* Design a pipeline to load individual videos and stream them to model in 3 second chunks \n",
    "    * load a file. split the audio file into 960 ms clips \n",
    "    * once we are out of clips we load another file\n",
    "* Import VGGish for embedding 1 second chunks\n",
    "* Stream data to the model and train LSTM + feed-forward \n",
    "* Visualize loss before hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class 1 examples: 76\n",
      "Number of class 0 examples: 23\n",
      "Total length of class 1 examples: 388.9250442176871\n",
      "Total length of class 0 examples: 586.8637993197277\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Number of class 1 examples:\", train[train['class'] == 1].shape[0])\n",
    "print(\"Number of class 0 examples:\", train[train['class'] == 0].shape[0])\n",
    "\n",
    "total_length_class_1 = train[train['class'] == 1]['length (minutes)'].sum()\n",
    "total_length_class_0 = train[train['class'] == 0]['length (minutes)'].sum()\n",
    "\n",
    "print(\"Total length of class 1 examples:\", total_length_class_1)\n",
    "print(\"Total length of class 0 examples:\", total_length_class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_pipeline.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish(\n",
      "  (features_network): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embedding_network): Sequential(\n",
      "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchaudio.prototype.pipelines import VGGISH\n",
    "\n",
    "# Get the pre-trained VGGish model\n",
    "vggish = VGGISH.get_model()\n",
    "\n",
    "print(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.fc3 = nn.Linear(8, output_size)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers with activation\n",
    "        x = self.fc1(last_output)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Apply final activation\n",
    "        return self.final_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "input_size = 128    # Output size of VGGish features\n",
    "hidden_size = 128   # hidden state size of the LSTM\n",
    "num_layers = 2      # Number of stacked LSTM layers\n",
    "output_size = 1     # Binary classification (positive/negative sentiment)\n",
    "dropout = 0.3       # Dropout for regularization\n",
    "\n",
    "model = SentimentLSTM(input_size, hidden_size, num_layers, output_size, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(audio_path, vggish, device=\"cpu\"):\n",
    "    sampler = torchaudio.transforms.Resample(orig_freq=44100, new_freq=16000).to(device)\n",
    "    spectrogram_converter = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_mels=64,        # Matches VGGish input\n",
    "        n_fft=400,        # 25 ms window size\n",
    "        hop_length=160,   # 10 ms hop size\n",
    "        f_min=125,        # Minimum frequency for mel filter bank\n",
    "        f_max=7500        # Maximum frequency for mel filter bank\n",
    "    ).to(device)\n",
    "\n",
    "    # Load audio file\n",
    "    audio_data, sr = torchaudio.load(audio_path)\n",
    "    audio_data = sampler(audio_data)  # Resample to 16 kHz\n",
    "    audio_data= spectrogram_converter(audio_data)  # Log mel spectrogram\n",
    "    audio_data = audio_data.mean(dim=0, keepdim=True) # convert to mono\n",
    "\n",
    "    # Extract VGGish features\n",
    "    features = []\n",
    "    for sample in audio_data.split(96, dim=2):\n",
    "        if sample.size(2) < 96:\n",
    "            padding = 96 - sample.size(2)\n",
    "            sample = torch.nn.functional.pad(sample, (0, padding), mode='constant', value=0)\n",
    "        sample = sample.unsqueeze(1).to(device)\n",
    "        features.append(vggish(sample.to(device)).squeeze(0))\n",
    "        \n",
    "    print(\"extracted features for\", audio_path)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, train, model, batch_size=32, device=\"cpu\"):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_func = nn.BCELoss()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        sample_count = 0\n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for idx, row in train.iterrows():\n",
    "            # Extract features and label\n",
    "            features = extract_features(row[\"path\"], vggish, device)\n",
    "            label = torch.tensor([row[\"class\"]], dtype=torch.float32).to(device)\n",
    "\n",
    "            for feature in features:\n",
    "                # Accumulate features and labels for batching\n",
    "                batch_features.append(feature.unsqueeze(0).unsqueeze(0))\n",
    "                batch_labels.append(label)\n",
    "\n",
    "                # If we reach batch size, process the batch\n",
    "                if len(batch_features) == batch_size:\n",
    "                    batch_features = torch.cat(batch_features).to(device)\n",
    "                    batch_labels = torch.cat(batch_labels).unsqueeze(1).to(device)\n",
    "\n",
    "                    # Compute predictions and loss\n",
    "                    predictions = model(batch_features)\n",
    "                    loss = loss_func(predictions, batch_labels)\n",
    "\n",
    "                    # Update model\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item() * batch_size\n",
    "                    sample_count += batch_size\n",
    "\n",
    "                    # Reset batch\n",
    "                    batch_features = []\n",
    "                    batch_labels = []\n",
    "\n",
    "\n",
    "                # Handle leftover features in the last batch\n",
    "                if batch_features:\n",
    "                    batch_features = torch.cat(batch_features).to(device)\n",
    "                    batch_labels = torch.cat(batch_labels).unsqueeze(1).to(device)\n",
    "\n",
    "                    predictions = model(batch_features)\n",
    "                    loss = loss_func(predictions, batch_labels)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item() * len(batch_features)\n",
    "                    sample_count += len(batch_features)\n",
    "\n",
    "                    # Reset batch\n",
    "                    batch_features = []\n",
    "                    batch_labels = []\n",
    "\n",
    "                avg_loss = total_loss / sample_count if sample_count > 0 else 0\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "                train_losses.append(avg_loss)\n",
    "\n",
    "                if idx > 0:\n",
    "                    break  # only train on one example for now\n",
    "\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted features for data/dangerous/ANOTHER FIGHT ON THE 2 TRAIN NYC SMFH LMFAO! @SHOTIMETV ON IG.wav\n",
      "Epoch 1/1, Loss: 12.959568\n",
      "Epoch 1/1, Loss: 12.833080\n",
      "Epoch 1/1, Loss: 12.623137\n",
      "Epoch 1/1, Loss: 12.419320\n",
      "Epoch 1/1, Loss: 12.286698\n",
      "Epoch 1/1, Loss: 12.143320\n",
      "Epoch 1/1, Loss: 11.984709\n",
      "Epoch 1/1, Loss: 11.806667\n",
      "Epoch 1/1, Loss: 11.655394\n",
      "Epoch 1/1, Loss: 11.417984\n",
      "Epoch 1/1, Loss: 11.245034\n",
      "Epoch 1/1, Loss: 11.094389\n",
      "Epoch 1/1, Loss: 10.943359\n",
      "Epoch 1/1, Loss: 10.793807\n",
      "Epoch 1/1, Loss: 10.654155\n",
      "Epoch 1/1, Loss: 10.508861\n",
      "Epoch 1/1, Loss: 10.347005\n",
      "Epoch 1/1, Loss: 10.175807\n",
      "Epoch 1/1, Loss: 10.018629\n",
      "Epoch 1/1, Loss: 9.816680\n",
      "Epoch 1/1, Loss: 9.641315\n",
      "Epoch 1/1, Loss: 9.488691\n",
      "Epoch 1/1, Loss: 9.330985\n",
      "Epoch 1/1, Loss: 9.171708\n",
      "Epoch 1/1, Loss: 8.991265\n",
      "Epoch 1/1, Loss: 8.817975\n",
      "Epoch 1/1, Loss: 8.649133\n",
      "Epoch 1/1, Loss: 8.488380\n",
      "Epoch 1/1, Loss: 8.318951\n",
      "Epoch 1/1, Loss: 8.154168\n",
      "Epoch 1/1, Loss: 7.993641\n",
      "Epoch 1/1, Loss: 7.808304\n",
      "Epoch 1/1, Loss: 7.633723\n",
      "Epoch 1/1, Loss: 7.466590\n",
      "Epoch 1/1, Loss: 7.301193\n",
      "Epoch 1/1, Loss: 7.140857\n",
      "Epoch 1/1, Loss: 6.981006\n",
      "Epoch 1/1, Loss: 6.830368\n",
      "Epoch 1/1, Loss: 6.683105\n",
      "Epoch 1/1, Loss: 6.544305\n",
      "Epoch 1/1, Loss: 6.403061\n",
      "Epoch 1/1, Loss: 6.269965\n",
      "Epoch 1/1, Loss: 6.142793\n",
      "Epoch 1/1, Loss: 6.020154\n",
      "Epoch 1/1, Loss: 5.901996\n",
      "Epoch 1/1, Loss: 5.789123\n",
      "Epoch 1/1, Loss: 5.680264\n",
      "Epoch 1/1, Loss: 5.575905\n",
      "Epoch 1/1, Loss: 5.475675\n",
      "Epoch 1/1, Loss: 5.379448\n",
      "Epoch 1/1, Loss: 5.286923\n",
      "Epoch 1/1, Loss: 5.197856\n",
      "Epoch 1/1, Loss: 5.112037\n",
      "Epoch 1/1, Loss: 5.029405\n",
      "Epoch 1/1, Loss: 4.949779\n",
      "Epoch 1/1, Loss: 4.872777\n",
      "Epoch 1/1, Loss: 4.798417\n",
      "Epoch 1/1, Loss: 4.726549\n",
      "Epoch 1/1, Loss: 4.657068\n",
      "Epoch 1/1, Loss: 4.589835\n",
      "Epoch 1/1, Loss: 4.524807\n",
      "Epoch 1/1, Loss: 4.461875\n",
      "Epoch 1/1, Loss: 4.400917\n",
      "Epoch 1/1, Loss: 4.341735\n",
      "Epoch 1/1, Loss: 4.284353\n",
      "Epoch 1/1, Loss: 4.228687\n",
      "Epoch 1/1, Loss: 4.174595\n",
      "Epoch 1/1, Loss: 4.122091\n",
      "Epoch 1/1, Loss: 4.071100\n",
      "Epoch 1/1, Loss: 4.021618\n",
      "Epoch 1/1, Loss: 3.973283\n",
      "Epoch 1/1, Loss: 3.926346\n",
      "Epoch 1/1, Loss: 3.880667\n",
      "Epoch 1/1, Loss: 3.835872\n",
      "Epoch 1/1, Loss: 3.792367\n",
      "Epoch 1/1, Loss: 3.749825\n",
      "Epoch 1/1, Loss: 3.708520\n",
      "Epoch 1/1, Loss: 3.668226\n",
      "Epoch 1/1, Loss: 3.628861\n",
      "Epoch 1/1, Loss: 3.590172\n",
      "Epoch 1/1, Loss: 3.552577\n",
      "Epoch 1/1, Loss: 3.515941\n",
      "Epoch 1/1, Loss: 3.479841\n",
      "Epoch 1/1, Loss: 3.444650\n",
      "Epoch 1/1, Loss: 3.409973\n",
      "Epoch 1/1, Loss: 3.376336\n",
      "Epoch 1/1, Loss: 3.343499\n",
      "Epoch 1/1, Loss: 3.311146\n",
      "Epoch 1/1, Loss: 3.279492\n",
      "Epoch 1/1, Loss: 3.248527\n",
      "Epoch 1/1, Loss: 3.217823\n",
      "Epoch 1/1, Loss: 3.187470\n",
      "Epoch 1/1, Loss: 3.158308\n",
      "Epoch 1/1, Loss: 3.129437\n",
      "Epoch 1/1, Loss: 3.101028\n",
      "Epoch 1/1, Loss: 3.072862\n",
      "Epoch 1/1, Loss: 3.045292\n",
      "Epoch 1/1, Loss: 3.017801\n",
      "Epoch 1/1, Loss: 2.991342\n",
      "Epoch 1/1, Loss: 2.964943\n",
      "Epoch 1/1, Loss: 2.938709\n",
      "Epoch 1/1, Loss: 2.912924\n",
      "Epoch 1/1, Loss: 2.888077\n",
      "Epoch 1/1, Loss: 2.863558\n",
      "Epoch 1/1, Loss: 2.839060\n",
      "Epoch 1/1, Loss: 2.814537\n",
      "Epoch 1/1, Loss: 2.790565\n",
      "Epoch 1/1, Loss: 2.767181\n",
      "Epoch 1/1, Loss: 2.743708\n",
      "Epoch 1/1, Loss: 2.720718\n",
      "Epoch 1/1, Loss: 2.697878\n",
      "Epoch 1/1, Loss: 2.675712\n",
      "Epoch 1/1, Loss: 2.653376\n",
      "Epoch 1/1, Loss: 2.632049\n",
      "Epoch 1/1, Loss: 2.610483\n",
      "Epoch 1/1, Loss: 2.589141\n",
      "Epoch 1/1, Loss: 2.568385\n",
      "Epoch 1/1, Loss: 2.547675\n",
      "Epoch 1/1, Loss: 2.527256\n",
      "Epoch 1/1, Loss: 2.507052\n",
      "Epoch 1/1, Loss: 2.487150\n",
      "Epoch 1/1, Loss: 2.467545\n",
      "Epoch 1/1, Loss: 2.448376\n",
      "Epoch 1/1, Loss: 2.429367\n",
      "Epoch 1/1, Loss: 2.410498\n",
      "Epoch 1/1, Loss: 2.391806\n",
      "Epoch 1/1, Loss: 2.373743\n",
      "Epoch 1/1, Loss: 2.355556\n",
      "Epoch 1/1, Loss: 2.337765\n",
      "Epoch 1/1, Loss: 2.320216\n",
      "Epoch 1/1, Loss: 2.302881\n",
      "Epoch 1/1, Loss: 2.285798\n",
      "Epoch 1/1, Loss: 2.268830\n",
      "Epoch 1/1, Loss: 2.252100\n",
      "Epoch 1/1, Loss: 2.235656\n",
      "Epoch 1/1, Loss: 2.219522\n",
      "Epoch 1/1, Loss: 2.203492\n",
      "Epoch 1/1, Loss: 2.187740\n",
      "Epoch 1/1, Loss: 2.172338\n",
      "Epoch 1/1, Loss: 2.156943\n",
      "Epoch 1/1, Loss: 2.141800\n",
      "Epoch 1/1, Loss: 2.126864\n",
      "Epoch 1/1, Loss: 2.112125\n",
      "Epoch 1/1, Loss: 2.097608\n",
      "Epoch 1/1, Loss: 2.083246\n",
      "Epoch 1/1, Loss: 2.069096\n",
      "Epoch 1/1, Loss: 2.055117\n",
      "Epoch 1/1, Loss: 2.041369\n",
      "Epoch 1/1, Loss: 2.027812\n",
      "Epoch 1/1, Loss: 2.014398\n",
      "Epoch 1/1, Loss: 2.001175\n",
      "Epoch 1/1, Loss: 1.988092\n",
      "Epoch 1/1, Loss: 1.975157\n",
      "Epoch 1/1, Loss: 1.962397\n",
      "Epoch 1/1, Loss: 1.949790\n",
      "Epoch 1/1, Loss: 1.937342\n",
      "Epoch 1/1, Loss: 1.925061\n",
      "Epoch 1/1, Loss: 1.912948\n",
      "Epoch 1/1, Loss: 1.900959\n",
      "Epoch 1/1, Loss: 1.889239\n",
      "Epoch 1/1, Loss: 1.877585\n",
      "Epoch 1/1, Loss: 1.866045\n",
      "Epoch 1/1, Loss: 1.854644\n",
      "Epoch 1/1, Loss: 1.843410\n",
      "Epoch 1/1, Loss: 1.832288\n",
      "Epoch 1/1, Loss: 1.821323\n",
      "Epoch 1/1, Loss: 1.810462\n",
      "Epoch 1/1, Loss: 1.799737\n",
      "Epoch 1/1, Loss: 1.789139\n",
      "Epoch 1/1, Loss: 1.778677\n",
      "Epoch 1/1, Loss: 1.768319\n",
      "Epoch 1/1, Loss: 1.758065\n",
      "Epoch 1/1, Loss: 1.747924\n",
      "Epoch 1/1, Loss: 1.737905\n",
      "Epoch 1/1, Loss: 1.727997\n",
      "Epoch 1/1, Loss: 1.718202\n",
      "Epoch 1/1, Loss: 1.708528\n",
      "Epoch 1/1, Loss: 1.698963\n",
      "Epoch 1/1, Loss: 1.689503\n",
      "Epoch 1/1, Loss: 1.680158\n",
      "Epoch 1/1, Loss: 1.670907\n",
      "Epoch 1/1, Loss: 1.661759\n",
      "Epoch 1/1, Loss: 1.652725\n",
      "Epoch 1/1, Loss: 1.643807\n",
      "Epoch 1/1, Loss: 1.634949\n",
      "Epoch 1/1, Loss: 1.626237\n",
      "Epoch 1/1, Loss: 1.617602\n",
      "Epoch 1/1, Loss: 1.609094\n",
      "Epoch 1/1, Loss: 1.600655\n",
      "Epoch 1/1, Loss: 1.592294\n",
      "Epoch 1/1, Loss: 1.583997\n",
      "Epoch 1/1, Loss: 1.575810\n",
      "Epoch 1/1, Loss: 1.567706\n",
      "extracted features for data/dangerous/VIDEO： Fight between Rapid bus driver and student.wav\n",
      "Epoch 1/1, Loss: 1.559706\n",
      "extracted features for data/safe/TTC Subway 123.wav\n",
      "Epoch 1/1, Loss: 1.580264\n",
      "extracted features for data/dangerous/Fight in NYC transit calling lame at 2 guys.wav\n",
      "Epoch 1/1, Loss: 1.572294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_losses)\n",
      "Cell \u001b[1;32mIn[127], line 16\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(epochs, train, model, batch_size, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Extract features and label\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvggish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Accumulate features and labels for batching\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[119], line 27\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(audio_path, vggish, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m         sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(sample, (\u001b[38;5;241m0\u001b[39m, padding), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 27\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvggish\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted features for\u001b[39m\u001b[38;5;124m\"\u001b[39m, audio_path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torchaudio\\prototype\\pipelines\\_vggish\\_vggish_impl.py:211\u001b[0m, in \u001b[0;36mVGGish.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        input (torch.Tensor): batch of spectrograms, with shape `(n_example, 1, n_frame, 64)`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m        torch.Tensor: model output, with shape `(n_example, 128)`.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2005e\\OneDrive\\Documents\\GitHub\\TTC-Listen-2\\env\\Lib\\site-packages\\torch\\nn\\functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = training_loop(1, train, model)\n",
    "print(train_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
